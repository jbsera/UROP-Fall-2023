{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Overall imports/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joyse\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the temp files\n",
    "temp_jan=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_01.dta\")\n",
    "temp_feb=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_02.dta\")\n",
    "temp_mar=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_03.dta\")\n",
    "temp_apr=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_04.dta\")\n",
    "temp_may=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_05.dta\")\n",
    "temp_june=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_06.dta\")\n",
    "temp_july=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_07.dta\")\n",
    "temp_aug=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_08.dta\")\n",
    "temp_sept=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_09.dta\")\n",
    "temp_oct=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_10.dta\")\n",
    "temp_nov=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_11.dta\")\n",
    "temp_dec=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_12.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hungary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Stata dataset into a DataFrame\n",
    "lat_long_match = pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\QGIS\\\\final_LatLong_match.dta\")\n",
    "hungary_MTUS=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Hungary.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all lat/long points in Spain\n",
    "hungary_match = lat_long_match[lat_long_match['name_0'] == 'Hungary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create country codes in MTUS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = dict(zip(hungary_match['hasc_1'], hungary_match['name_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HU.SO': 'Somogy',\n",
       " 'HU.BA': 'Baranya',\n",
       " 'HU.TO': 'Tolna',\n",
       " 'HU.BK': 'Bács-Kiskun',\n",
       " 'HU.CS': 'Csongrád',\n",
       " 'HU.ZA': 'Zala',\n",
       " 'HU.BE': 'Békés',\n",
       " 'HU.VA': 'Vas',\n",
       " 'HU.VE': 'Veszprém',\n",
       " 'HU.FE': 'Fejér',\n",
       " 'HU.PE': 'Pest',\n",
       " 'HU.JN': 'Jász-Nagykun-Szolnok',\n",
       " 'HU.HB': 'Hajdú-Bihar',\n",
       " 'HU.GS': 'Gyor-Moson-Sopron',\n",
       " 'HU.KE': 'Komárom-Esztergom',\n",
       " 'HU.HE': 'Heves',\n",
       " 'HU.BZ': 'Borsod-Abaúj-Zemplén',\n",
       " 'HU.SZ': 'Szabolcs-Szatmár-Bereg'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print dictionary to verify\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['Somogy', 'Baranya', 'Tolna', 'Bács-Kiskun', 'Csongrád', 'Zala', 'Békés', 'Vas', 'Veszprém', 'Fejér', 'Pest', 'Jász-Nagykun-Szolnok', 'Hajdú-Bihar', 'Gyor-Moson-Sopron', 'Komárom-Esztergom', 'Heves', 'Borsod-Abaúj-Zemplén', 'Szabolcs-Szatmár-Bereg'])\n"
     ]
    }
   ],
   "source": [
    "print(mapping_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rectype', 'sample', 'ident', 'country', 'hldid', 'persid', 'diary',\n",
       "       'year', 'diaryid', 'day', 'month', 'hhtype', 'hhldsize', 'nchild',\n",
       "       'famstat', 'singpar', 'ownhome', 'vehicle', 'computer', 'urban',\n",
       "       'agekid_hu', 'age', 'sex', 'citizen', 'civstat', 'cohab', 'edtry',\n",
       "       'educa', 'empstat', 'empsp', 'sector', 'emp', 'unemp', 'workhrs',\n",
       "       'retired', 'student', 'health', 'region', 'disab', 'income', 'incorig',\n",
       "       'occupo', 'isco1', 'ocombwt', 'propwt', 'region_hu', 'diarytype',\n",
       "       'badcase', 'act_chcare', 'act_civic', 'act_educa', 'act_inhome',\n",
       "       'act_media', 'act_norec', 'act_outhome', 'act_pcare', 'act_physical',\n",
       "       'act_travel', 'act_undom', 'act_work', 'cday', 'epnum', 'clockst',\n",
       "       'start', 'end', 'time', 'main', 'sec', 'eloc', 'inout', 'mtrav',\n",
       "       'alone', 'alone_alt', 'child', 'sppart', 'oad', 'ict'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hungary_MTUS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region_hu\n",
       "Budapest (NUTS 3)                  1141\n",
       "Pest (NUTS 3)                       812\n",
       "Borsod-Abaúj-Zemplén (NUTS 3)       705\n",
       "Szabolcs-Szatmár-Bereg (NUTS 3)     538\n",
       "Hajdú-Bihar (NUTS 3)                520\n",
       "Bács-Kiskun (NUTS 3)                506\n",
       "Békés (NUTS 3)                      394\n",
       "Jász-Nagykun-Szolnok (NUTS 3)       382\n",
       "Baranya (NUTS 3)                    353\n",
       "Fejér (NUTS 3)                      342\n",
       "Csongrád (NUTS 3)                   337\n",
       "Gyõr-Moson-Sopron (NUTS 3)          311\n",
       "Veszprém (NUTS 3)                   310\n",
       "Heves (NUTS 3)                      277\n",
       "Somogy (NUTS 3)                     265\n",
       "Zala (NUTS 3)                       264\n",
       "Komárom-Esztergom (NUTS 3)          263\n",
       "Tolna (NUTS 3)                      228\n",
       "Vas (NUTS 3)                        226\n",
       "Nógrád (NUTS 3)                     216\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the regions in the Spain MTUS dataset\n",
    "hungary_MTUS['region_hu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Somogy': 'HU.SO',\n",
       " 'Baranya': 'HU.BA',\n",
       " 'Tolna': 'HU.TO',\n",
       " 'Bács-Kiskun': 'HU.BK',\n",
       " 'Csongrád': 'HU.CS',\n",
       " 'Zala': 'HU.ZA',\n",
       " 'Békés': 'HU.BE',\n",
       " 'Vas': 'HU.VA',\n",
       " 'Veszprém': 'HU.VE',\n",
       " 'Fejér': 'HU.FE',\n",
       " 'Pest': 'HU.PE',\n",
       " 'Jász-Nagykun-Szolnok': 'HU.JN',\n",
       " 'Hajdú-Bihar': 'HU.HB',\n",
       " 'Gyor-Moson-Sopron': 'HU.GS',\n",
       " 'Komárom-Esztergom': 'HU.KE',\n",
       " 'Heves': 'HU.HE',\n",
       " 'Borsod-Abaúj-Zemplén': 'HU.BZ',\n",
       " 'Szabolcs-Szatmár-Bereg': 'HU.SZ'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flip the value of our earlier dictionary for easier matching later\n",
    "flipped_dict = {value: key for key, value in mapping_dict.items()}\n",
    "flipped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edditing the final dict based on the syntax from spain_MTUS\n",
    "final_dict={'Pest (NUTS 3)': 'HU.PE', 'Borsod-Abaúj-Zemplén (NUTS 3)': 'HU.BZ', 'Szabolcs-Szatmár-Bereg (NUTS 3)': 'HU.SZ', \n",
    "            'Hajdú-Bihar (NUTS 3)': 'HU.HB', 'Bács-Kiskun (NUTS 3)': 'HU.BK', 'Békés (NUTS 3)': 'HU.BE',\n",
    "            'Jász-Nagykun-Szolnok (NUTS 3)': 'HU.JN', 'Baranya (NUTS 3)': 'HU.BA', 'Fejér (NUTS 3)': 'HU.FE',\n",
    "            'Csongrád (NUTS 3)': 'HU.CS', 'Gyõr-Moson-Sopron (NUTS 3)': 'HU.GS', 'Veszprém (NUTS 3)': 'HU.VE', \n",
    "            'Heves (NUTS 3)': 'HU.HE', 'Somogy (NUTS 3)': 'HU.SO', 'Zala (NUTS 3)': 'HU.ZA', 'Komárom-Esztergom (NUTS 3)': 'HU.KE', \n",
    "            'Tolna (NUTS 3)': 'HU.TO', 'Vas (NUTS 3)': 'HU.VA', 'Budapest (NUTS 3)': 'HU.PE', 'Nógrád (NUTS 3)': 'HU.HE'\n",
    "           }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new variable in Spain MTUS with the corresponding abbreviation, useful for later merge\n",
    "hungary_MTUS['region_hu_code'] = hungary_MTUS['region_hu'].map(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the start time to base 10\n",
    "hungary_MTUS['clockst_base10'] = hungary_MTUS['clockst'].apply(lambda x: int(x) + (x % 1) / 0.6 if not pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all lat/long points by location and merge temperature data into MTUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the lat, long values within a given region\n",
    "points_by_state = {}\n",
    "\n",
    "# Iterate over each state code in the dictionary\n",
    "for state_code in mapping_dict.keys():\n",
    "    # Filter the DataFrame for points in the current region\n",
    "    region_points = hungary_match.loc[hungary_match['hasc_1'] == state_code, ['latitude', 'longitude']]\n",
    "    \n",
    "    # Convert the filtered DataFrame to a list of tuples\n",
    "    points_list = list(zip(region_points['latitude'], region_points['longitude']))\n",
    "    \n",
    "    # Store the list of points in the dictionary\n",
    "    points_by_state[state_code] = points_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HU.SO': [(46.25, 17.25), (46.25, 17.75), (46.75, 17.75)],\n",
       " 'HU.BA': [(46.25, 18.25)],\n",
       " 'HU.TO': [(46.25, 18.75), (46.75, 18.25), (46.75, 18.75)],\n",
       " 'HU.BK': [(46.25, 19.25), (46.75, 19.25), (46.75, 19.75)],\n",
       " 'HU.CS': [(46.25, 19.75), (46.25, 20.25), (46.25, 20.75), (46.75, 20.25)],\n",
       " 'HU.ZA': [(46.75, 16.75), (46.75, 17.25)],\n",
       " 'HU.BE': [(46.75, 20.75), (46.75, 21.25)],\n",
       " 'HU.VA': [(47.25, 16.75)],\n",
       " 'HU.VE': [(47.25, 17.25), (47.25, 17.75)],\n",
       " 'HU.FE': [(47.25, 18.25), (47.25, 18.75)],\n",
       " 'HU.PE': [(47.25, 19.25), (47.25, 19.75), (47.75, 19.25)],\n",
       " 'HU.JN': [(47.25, 20.25), (47.25, 20.75)],\n",
       " 'HU.HB': [(47.25, 21.25), (47.25, 21.75), (47.75, 21.25)],\n",
       " 'HU.GS': [(47.75, 17.25), (47.75, 17.75)],\n",
       " 'HU.KE': [(47.75, 18.75)],\n",
       " 'HU.HE': [(47.75, 19.75), (47.75, 20.25)],\n",
       " 'HU.BZ': [(47.75, 20.75),\n",
       "  (48.25, 20.25),\n",
       "  (48.25, 20.75),\n",
       "  (48.25, 21.25),\n",
       "  (48.25, 21.75)],\n",
       " 'HU.SZ': [(47.75, 21.75), (47.75, 22.25), (48.25, 22.25)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display points_by_state dictionary to verify\n",
    "points_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells get the average temperature for specific months of surveys, if monthly info not available, proceed to next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for January\n",
    "year=2003\n",
    "month=\"January\"\n",
    "temps_by_region_jan = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_jan', \"avg_r_jan\", \n",
    "                                       \"avg_iqr_jan\", \"avg_sd_jan\", \n",
    "                                        \"avg_n30_jan\", \"avg_n35_jan\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_jan[(temp_jan['latitude'] == lat) & \n",
    "                             (temp_jan['longitude'] == long) & \n",
    "                             (temp_jan['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month1'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            #num_above_30_list.extend(temp_data['wavg_25_ct'].tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_jan.loc[len(temps_by_region_jan.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for April\n",
    "year=2003\n",
    "month=\"April\"\n",
    "temps_by_region_apr = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_apr', \"avg_r_apr\", \n",
    "                                       \"avg_iqr_apr\", \"avg_sd_apr\", \n",
    "                                        \"avg_n30_apr\", \"avg_n35_apr\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_apr[(temp_apr['latitude'] == lat) & \n",
    "                             (temp_apr['longitude'] == long) & \n",
    "                             (temp_apr['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month4'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_apr.loc[len(temps_by_region_apr.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for October\n",
    "year=2002\n",
    "month=\"October\"\n",
    "temps_by_region_oct = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_oct', \"avg_r_oct\", \n",
    "                                       \"avg_iqr_oct\", \"avg_sd_oct\", \n",
    "                                        \"avg_n30_oct\", \"avg_n35_oct\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_oct[(temp_oct['latitude'] == lat) & \n",
    "                             (temp_oct['longitude'] == long) & \n",
    "                             (temp_oct['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month10'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_oct.loc[len(temps_by_region_oct.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2003\n",
    "month=\"July\"\n",
    "temps_by_region_july = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_july', \"avg_r_july\", \n",
    "                                       \"avg_iqr_july\", \"avg_sd_july\", \n",
    "                                        \"avg_n30_july\", \"avg_n35_july\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_july[(temp_july['latitude'] == lat) & \n",
    "                             (temp_july['longitude'] == long) & \n",
    "                             (temp_july['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month7'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "\n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_july.loc[len(temps_by_region_july.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the results based on region, year, and month\n",
    "spain_MTUS['region_sp_code_str'] = spain_MTUS['region_sp_code'].astype(str)\n",
    "spain_MTUS['month_str'] = spain_MTUS['month'].astype(str)\n",
    "spain_MTUS['year_str'] = spain_MTUS['year'].astype(str)\n",
    "\n",
    "\n",
    "merged_spain_MTUS = spain_MTUS.merge(temps_by_region_jan, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_apr, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_july, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_oct, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following gets the temperature stats for the year(s) of the survey, not the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=1999\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_1999 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_1999', \"avg_r_1999\", \n",
    "                                       \"avg_iqr_1999\", \"avg_sd_1999\", \n",
    "                                        \"avg_n30_1999\", \"avg_n35_1999\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_1999.loc[len(temps_by_region_1999.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_temp_1999</th>\n",
       "      <th>avg_r_1999</th>\n",
       "      <th>avg_iqr_1999</th>\n",
       "      <th>avg_sd_1999</th>\n",
       "      <th>avg_n30_1999</th>\n",
       "      <th>avg_n35_1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HU.SO</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.110523</td>\n",
       "      <td>6.829923</td>\n",
       "      <td>4.336532</td>\n",
       "      <td>2.345620</td>\n",
       "      <td>2.638889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HU.BA</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.923823</td>\n",
       "      <td>7.088332</td>\n",
       "      <td>4.589131</td>\n",
       "      <td>2.445851</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HU.TO</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.987149</td>\n",
       "      <td>6.962762</td>\n",
       "      <td>4.483217</td>\n",
       "      <td>2.403816</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HU.BK</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.184440</td>\n",
       "      <td>7.342913</td>\n",
       "      <td>4.747691</td>\n",
       "      <td>2.530669</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HU.CS</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.515517</td>\n",
       "      <td>7.475186</td>\n",
       "      <td>4.799665</td>\n",
       "      <td>2.570882</td>\n",
       "      <td>4.937500</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HU.ZA</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.837871</td>\n",
       "      <td>6.823545</td>\n",
       "      <td>4.285825</td>\n",
       "      <td>2.336935</td>\n",
       "      <td>2.208333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HU.BE</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.420059</td>\n",
       "      <td>7.455276</td>\n",
       "      <td>4.757518</td>\n",
       "      <td>2.553212</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HU.VA</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.214043</td>\n",
       "      <td>7.061032</td>\n",
       "      <td>4.521989</td>\n",
       "      <td>2.444287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HU.VE</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.460800</td>\n",
       "      <td>6.606655</td>\n",
       "      <td>4.202931</td>\n",
       "      <td>2.276265</td>\n",
       "      <td>1.541667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HU.FE</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.414496</td>\n",
       "      <td>6.815376</td>\n",
       "      <td>4.395505</td>\n",
       "      <td>2.358058</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HU.PE</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.525555</td>\n",
       "      <td>7.174421</td>\n",
       "      <td>4.612756</td>\n",
       "      <td>2.466587</td>\n",
       "      <td>1.444444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HU.JN</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.144718</td>\n",
       "      <td>7.326671</td>\n",
       "      <td>4.658901</td>\n",
       "      <td>2.501410</td>\n",
       "      <td>2.583333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HU.HB</td>\n",
       "      <td>1999</td>\n",
       "      <td>11.014808</td>\n",
       "      <td>7.359263</td>\n",
       "      <td>4.700619</td>\n",
       "      <td>2.519208</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HU.GS</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.786472</td>\n",
       "      <td>6.851984</td>\n",
       "      <td>4.387625</td>\n",
       "      <td>2.372216</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HU.KE</td>\n",
       "      <td>1999</td>\n",
       "      <td>9.972154</td>\n",
       "      <td>7.168369</td>\n",
       "      <td>4.686725</td>\n",
       "      <td>2.484001</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HU.HE</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.026628</td>\n",
       "      <td>7.327006</td>\n",
       "      <td>4.683515</td>\n",
       "      <td>2.511628</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HU.BZ</td>\n",
       "      <td>1999</td>\n",
       "      <td>9.645092</td>\n",
       "      <td>7.431122</td>\n",
       "      <td>4.761057</td>\n",
       "      <td>2.544492</td>\n",
       "      <td>1.283333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HU.SZ</td>\n",
       "      <td>1999</td>\n",
       "      <td>10.534301</td>\n",
       "      <td>7.476802</td>\n",
       "      <td>4.798305</td>\n",
       "      <td>2.572343</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Year  avg_temp_1999  avg_r_1999  avg_iqr_1999  avg_sd_1999  \\\n",
       "0   HU.SO  1999      11.110523    6.829923      4.336532     2.345620   \n",
       "1   HU.BA  1999      10.923823    7.088332      4.589131     2.445851   \n",
       "2   HU.TO  1999      10.987149    6.962762      4.483217     2.403816   \n",
       "3   HU.BK  1999      11.184440    7.342913      4.747691     2.530669   \n",
       "4   HU.CS  1999      11.515517    7.475186      4.799665     2.570882   \n",
       "5   HU.ZA  1999      10.837871    6.823545      4.285825     2.336935   \n",
       "6   HU.BE  1999      11.420059    7.455276      4.757518     2.553212   \n",
       "7   HU.VA  1999      10.214043    7.061032      4.521989     2.444287   \n",
       "8   HU.VE  1999      10.460800    6.606655      4.202931     2.276265   \n",
       "9   HU.FE  1999      10.414496    6.815376      4.395505     2.358058   \n",
       "10  HU.PE  1999      10.525555    7.174421      4.612756     2.466587   \n",
       "11  HU.JN  1999      11.144718    7.326671      4.658901     2.501410   \n",
       "12  HU.HB  1999      11.014808    7.359263      4.700619     2.519208   \n",
       "13  HU.GS  1999      10.786472    6.851984      4.387625     2.372216   \n",
       "14  HU.KE  1999       9.972154    7.168369      4.686725     2.484001   \n",
       "15  HU.HE  1999      10.026628    7.327006      4.683515     2.511628   \n",
       "16  HU.BZ  1999       9.645092    7.431122      4.761057     2.544492   \n",
       "17  HU.SZ  1999      10.534301    7.476802      4.798305     2.572343   \n",
       "\n",
       "    avg_n30_1999  avg_n35_1999  \n",
       "0       2.638889      0.000000  \n",
       "1       2.500000      0.000000  \n",
       "2       2.250000      0.000000  \n",
       "3       3.083333      0.000000  \n",
       "4       4.937500      0.166667  \n",
       "5       2.208333      0.000000  \n",
       "6       3.875000      0.000000  \n",
       "7       1.000000      0.000000  \n",
       "8       1.541667      0.000000  \n",
       "9       1.375000      0.000000  \n",
       "10      1.444444      0.000000  \n",
       "11      2.583333      0.000000  \n",
       "12      2.888889      0.000000  \n",
       "13      1.625000      0.000000  \n",
       "14      0.500000      0.000000  \n",
       "15      0.791667      0.000000  \n",
       "16      1.283333      0.000000  \n",
       "17      3.500000      0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps_by_region_1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2000\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_2000 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_2000', \"avg_r_2000\", \n",
    "                                       \"avg_iqr_2000\", \"avg_sd_2000\", \n",
    "                                        \"avg_n30_2000\", \"avg_n35_2000\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_2000.loc[len(temps_by_region_2000.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_temp_2000</th>\n",
       "      <th>avg_r_2000</th>\n",
       "      <th>avg_iqr_2000</th>\n",
       "      <th>avg_sd_2000</th>\n",
       "      <th>avg_n30_2000</th>\n",
       "      <th>avg_n35_2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HU.SO</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.718204</td>\n",
       "      <td>7.871407</td>\n",
       "      <td>5.042250</td>\n",
       "      <td>2.721644</td>\n",
       "      <td>16.416667</td>\n",
       "      <td>1.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HU.BA</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.581020</td>\n",
       "      <td>8.155164</td>\n",
       "      <td>5.246698</td>\n",
       "      <td>2.832919</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HU.TO</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.530869</td>\n",
       "      <td>8.105146</td>\n",
       "      <td>5.271885</td>\n",
       "      <td>2.824732</td>\n",
       "      <td>16.638889</td>\n",
       "      <td>1.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HU.BK</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.749595</td>\n",
       "      <td>8.632631</td>\n",
       "      <td>5.612754</td>\n",
       "      <td>3.002293</td>\n",
       "      <td>21.805556</td>\n",
       "      <td>2.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HU.CS</td>\n",
       "      <td>2000</td>\n",
       "      <td>13.045348</td>\n",
       "      <td>8.847020</td>\n",
       "      <td>5.690325</td>\n",
       "      <td>3.059842</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HU.ZA</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.304866</td>\n",
       "      <td>7.698790</td>\n",
       "      <td>4.872124</td>\n",
       "      <td>2.642529</td>\n",
       "      <td>10.958333</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HU.BE</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.742959</td>\n",
       "      <td>8.643420</td>\n",
       "      <td>5.558426</td>\n",
       "      <td>2.985897</td>\n",
       "      <td>21.791667</td>\n",
       "      <td>2.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HU.VA</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.500047</td>\n",
       "      <td>7.997090</td>\n",
       "      <td>5.192727</td>\n",
       "      <td>2.772866</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HU.VE</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.890789</td>\n",
       "      <td>7.359627</td>\n",
       "      <td>4.680698</td>\n",
       "      <td>2.540734</td>\n",
       "      <td>10.166667</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HU.FE</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.790540</td>\n",
       "      <td>7.797709</td>\n",
       "      <td>5.092902</td>\n",
       "      <td>2.716796</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HU.PE</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.781741</td>\n",
       "      <td>8.045030</td>\n",
       "      <td>5.276867</td>\n",
       "      <td>2.801260</td>\n",
       "      <td>13.805556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HU.JN</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.412334</td>\n",
       "      <td>8.401099</td>\n",
       "      <td>5.449428</td>\n",
       "      <td>2.909998</td>\n",
       "      <td>20.041667</td>\n",
       "      <td>2.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HU.HB</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.152843</td>\n",
       "      <td>8.360843</td>\n",
       "      <td>5.386339</td>\n",
       "      <td>2.882366</td>\n",
       "      <td>18.833333</td>\n",
       "      <td>2.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HU.GS</td>\n",
       "      <td>2000</td>\n",
       "      <td>12.141318</td>\n",
       "      <td>7.550351</td>\n",
       "      <td>4.848790</td>\n",
       "      <td>2.613191</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HU.KE</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.122618</td>\n",
       "      <td>7.862386</td>\n",
       "      <td>5.224041</td>\n",
       "      <td>2.753729</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HU.HE</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.203408</td>\n",
       "      <td>8.118001</td>\n",
       "      <td>5.309496</td>\n",
       "      <td>2.817651</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HU.BZ</td>\n",
       "      <td>2000</td>\n",
       "      <td>10.652793</td>\n",
       "      <td>8.332420</td>\n",
       "      <td>5.339374</td>\n",
       "      <td>2.854983</td>\n",
       "      <td>10.383333</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HU.SZ</td>\n",
       "      <td>2000</td>\n",
       "      <td>11.480459</td>\n",
       "      <td>8.498734</td>\n",
       "      <td>5.452822</td>\n",
       "      <td>2.923107</td>\n",
       "      <td>17.138889</td>\n",
       "      <td>2.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Year  avg_temp_2000  avg_r_2000  avg_iqr_2000  avg_sd_2000  \\\n",
       "0   HU.SO  2000      12.718204    7.871407      5.042250     2.721644   \n",
       "1   HU.BA  2000      12.581020    8.155164      5.246698     2.832919   \n",
       "2   HU.TO  2000      12.530869    8.105146      5.271885     2.824732   \n",
       "3   HU.BK  2000      12.749595    8.632631      5.612754     3.002293   \n",
       "4   HU.CS  2000      13.045348    8.847020      5.690325     3.059842   \n",
       "5   HU.ZA  2000      12.304866    7.698790      4.872124     2.642529   \n",
       "6   HU.BE  2000      12.742959    8.643420      5.558426     2.985897   \n",
       "7   HU.VA  2000      11.500047    7.997090      5.192727     2.772866   \n",
       "8   HU.VE  2000      11.890789    7.359627      4.680698     2.540734   \n",
       "9   HU.FE  2000      11.790540    7.797709      5.092902     2.716796   \n",
       "10  HU.PE  2000      11.781741    8.045030      5.276867     2.801260   \n",
       "11  HU.JN  2000      12.412334    8.401099      5.449428     2.909998   \n",
       "12  HU.HB  2000      12.152843    8.360843      5.386339     2.882366   \n",
       "13  HU.GS  2000      12.141318    7.550351      4.848790     2.613191   \n",
       "14  HU.KE  2000      11.122618    7.862386      5.224041     2.753729   \n",
       "15  HU.HE  2000      11.203408    8.118001      5.309496     2.817651   \n",
       "16  HU.BZ  2000      10.652793    8.332420      5.339374     2.854983   \n",
       "17  HU.SZ  2000      11.480459    8.498734      5.452822     2.923107   \n",
       "\n",
       "    avg_n30_2000  avg_n35_2000  \n",
       "0      16.416667      1.611111  \n",
       "1      16.166667      1.666667  \n",
       "2      16.638889      1.611111  \n",
       "3      21.805556      2.138889  \n",
       "4      25.000000      3.125000  \n",
       "5      10.958333      0.541667  \n",
       "6      21.791667      2.583333  \n",
       "7       6.750000      0.000000  \n",
       "8      10.166667      0.125000  \n",
       "9      12.750000      0.875000  \n",
       "10     13.805556      1.000000  \n",
       "11     20.041667      2.416667  \n",
       "12     18.833333      2.055556  \n",
       "13     12.000000      0.958333  \n",
       "14      9.500000      0.333333  \n",
       "15     12.500000      0.625000  \n",
       "16     10.383333      0.533333  \n",
       "17     17.138889      2.083333  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps_by_region_2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2009\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_2009 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_2009', \"avg_r_2009\", \n",
    "                                       \"avg_iqr_2009\", \"avg_sd_2009\", \n",
    "                                        \"avg_n30_2009\", \"avg_n35_2009\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_2009.loc[len(temps_by_region_2009.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_temp_2009</th>\n",
       "      <th>avg_r_2009</th>\n",
       "      <th>avg_iqr_2009</th>\n",
       "      <th>avg_sd_2009</th>\n",
       "      <th>avg_n30_2009</th>\n",
       "      <th>avg_n35_2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HU.SO</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.149231</td>\n",
       "      <td>7.257067</td>\n",
       "      <td>4.616924</td>\n",
       "      <td>2.501509</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HU.BA</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.071007</td>\n",
       "      <td>7.513451</td>\n",
       "      <td>4.863714</td>\n",
       "      <td>2.601287</td>\n",
       "      <td>7.916667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HU.TO</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.243100</td>\n",
       "      <td>7.589183</td>\n",
       "      <td>4.915629</td>\n",
       "      <td>2.637186</td>\n",
       "      <td>9.722222</td>\n",
       "      <td>0.194444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HU.BK</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.489731</td>\n",
       "      <td>7.980773</td>\n",
       "      <td>5.237942</td>\n",
       "      <td>2.782582</td>\n",
       "      <td>13.055556</td>\n",
       "      <td>0.305556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HU.CS</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.722660</td>\n",
       "      <td>8.120810</td>\n",
       "      <td>5.360405</td>\n",
       "      <td>2.829284</td>\n",
       "      <td>14.562500</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HU.ZA</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.706198</td>\n",
       "      <td>7.148918</td>\n",
       "      <td>4.531771</td>\n",
       "      <td>2.464285</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HU.BE</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.560606</td>\n",
       "      <td>8.042867</td>\n",
       "      <td>5.276141</td>\n",
       "      <td>2.790108</td>\n",
       "      <td>11.958333</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HU.VA</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.049074</td>\n",
       "      <td>7.309773</td>\n",
       "      <td>4.745047</td>\n",
       "      <td>2.550076</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HU.VE</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.376618</td>\n",
       "      <td>6.990238</td>\n",
       "      <td>4.460852</td>\n",
       "      <td>2.421385</td>\n",
       "      <td>3.791667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HU.FE</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.553877</td>\n",
       "      <td>7.403114</td>\n",
       "      <td>4.761524</td>\n",
       "      <td>2.566144</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HU.PE</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.676648</td>\n",
       "      <td>7.669711</td>\n",
       "      <td>4.998226</td>\n",
       "      <td>2.667258</td>\n",
       "      <td>8.388889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HU.JN</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.362432</td>\n",
       "      <td>7.929416</td>\n",
       "      <td>5.180269</td>\n",
       "      <td>2.752824</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HU.HB</td>\n",
       "      <td>2009</td>\n",
       "      <td>12.185940</td>\n",
       "      <td>7.974966</td>\n",
       "      <td>5.167352</td>\n",
       "      <td>2.750601</td>\n",
       "      <td>12.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HU.GS</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.662099</td>\n",
       "      <td>7.155123</td>\n",
       "      <td>4.628679</td>\n",
       "      <td>2.492637</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HU.KE</td>\n",
       "      <td>2009</td>\n",
       "      <td>10.875565</td>\n",
       "      <td>7.639609</td>\n",
       "      <td>4.942027</td>\n",
       "      <td>2.645876</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HU.HE</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.122822</td>\n",
       "      <td>7.745777</td>\n",
       "      <td>5.043530</td>\n",
       "      <td>2.683521</td>\n",
       "      <td>5.458333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HU.BZ</td>\n",
       "      <td>2009</td>\n",
       "      <td>10.751795</td>\n",
       "      <td>7.905929</td>\n",
       "      <td>5.094312</td>\n",
       "      <td>2.723079</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HU.SZ</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.594922</td>\n",
       "      <td>8.171008</td>\n",
       "      <td>5.253868</td>\n",
       "      <td>2.811806</td>\n",
       "      <td>10.611111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Year  avg_temp_2009  avg_r_2009  avg_iqr_2009  avg_sd_2009  \\\n",
       "0   HU.SO  2009      12.149231    7.257067      4.616924     2.501509   \n",
       "1   HU.BA  2009      12.071007    7.513451      4.863714     2.601287   \n",
       "2   HU.TO  2009      12.243100    7.589183      4.915629     2.637186   \n",
       "3   HU.BK  2009      12.489731    7.980773      5.237942     2.782582   \n",
       "4   HU.CS  2009      12.722660    8.120810      5.360405     2.829284   \n",
       "5   HU.ZA  2009      11.706198    7.148918      4.531771     2.464285   \n",
       "6   HU.BE  2009      12.560606    8.042867      5.276141     2.790108   \n",
       "7   HU.VA  2009      11.049074    7.309773      4.745047     2.550076   \n",
       "8   HU.VE  2009      11.376618    6.990238      4.460852     2.421385   \n",
       "9   HU.FE  2009      11.553877    7.403114      4.761524     2.566144   \n",
       "10  HU.PE  2009      11.676648    7.669711      4.998226     2.667258   \n",
       "11  HU.JN  2009      12.362432    7.929416      5.180269     2.752824   \n",
       "12  HU.HB  2009      12.185940    7.974966      5.167352     2.750601   \n",
       "13  HU.GS  2009      11.662099    7.155123      4.628679     2.492637   \n",
       "14  HU.KE  2009      10.875565    7.639609      4.942027     2.645876   \n",
       "15  HU.HE  2009      11.122822    7.745777      5.043530     2.683521   \n",
       "16  HU.BZ  2009      10.751795    7.905929      5.094312     2.723079   \n",
       "17  HU.SZ  2009      11.594922    8.171008      5.253868     2.811806   \n",
       "\n",
       "    avg_n30_2009  avg_n35_2009  \n",
       "0       8.250000      0.138889  \n",
       "1       7.916667      0.166667  \n",
       "2       9.722222      0.194444  \n",
       "3      13.055556      0.305556  \n",
       "4      14.562500      0.354167  \n",
       "5       4.875000      0.000000  \n",
       "6      11.958333      0.041667  \n",
       "7       2.666667      0.000000  \n",
       "8       3.791667      0.000000  \n",
       "9       6.333333      0.000000  \n",
       "10      8.388889      0.000000  \n",
       "11     12.666667      0.000000  \n",
       "12     12.250000      0.000000  \n",
       "13      5.666667      0.000000  \n",
       "14      3.750000      0.000000  \n",
       "15      5.458333      0.000000  \n",
       "16      6.100000      0.000000  \n",
       "17     10.611111      0.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps_by_region_2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2010\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_2010 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_2010', \"avg_r_2010\", \n",
    "                                       \"avg_iqr_2010\", \"avg_sd_2010\", \n",
    "                                        \"avg_n30_2010\", \"avg_n35_2010\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_2010.loc[len(temps_by_region_2010.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Year</th>\n",
       "      <th>avg_temp_2010</th>\n",
       "      <th>avg_r_2010</th>\n",
       "      <th>avg_iqr_2010</th>\n",
       "      <th>avg_sd_2010</th>\n",
       "      <th>avg_n30_2010</th>\n",
       "      <th>avg_n35_2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HU.SO</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.636482</td>\n",
       "      <td>6.576395</td>\n",
       "      <td>4.133575</td>\n",
       "      <td>2.259847</td>\n",
       "      <td>6.388889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HU.BA</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.518564</td>\n",
       "      <td>6.734973</td>\n",
       "      <td>4.366267</td>\n",
       "      <td>2.340008</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HU.TO</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.607542</td>\n",
       "      <td>6.669679</td>\n",
       "      <td>4.356648</td>\n",
       "      <td>2.323706</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HU.BK</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.859338</td>\n",
       "      <td>7.067929</td>\n",
       "      <td>4.647409</td>\n",
       "      <td>2.459515</td>\n",
       "      <td>8.194444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HU.CS</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.291382</td>\n",
       "      <td>7.260301</td>\n",
       "      <td>4.732937</td>\n",
       "      <td>2.514615</td>\n",
       "      <td>10.854167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HU.ZA</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.451497</td>\n",
       "      <td>6.604736</td>\n",
       "      <td>4.129944</td>\n",
       "      <td>2.265031</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HU.BE</td>\n",
       "      <td>2010</td>\n",
       "      <td>11.307888</td>\n",
       "      <td>7.282594</td>\n",
       "      <td>4.690013</td>\n",
       "      <td>2.512147</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HU.VA</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.911473</td>\n",
       "      <td>6.855250</td>\n",
       "      <td>4.393657</td>\n",
       "      <td>2.369735</td>\n",
       "      <td>6.416667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HU.VE</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.047694</td>\n",
       "      <td>6.400322</td>\n",
       "      <td>4.051843</td>\n",
       "      <td>2.200229</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HU.FE</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.988019</td>\n",
       "      <td>6.603472</td>\n",
       "      <td>4.314282</td>\n",
       "      <td>2.292029</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HU.PE</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.121066</td>\n",
       "      <td>6.953152</td>\n",
       "      <td>4.482567</td>\n",
       "      <td>2.392689</td>\n",
       "      <td>4.944444</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HU.JN</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.835110</td>\n",
       "      <td>7.106805</td>\n",
       "      <td>4.554490</td>\n",
       "      <td>2.447855</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HU.HB</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.831492</td>\n",
       "      <td>7.063108</td>\n",
       "      <td>4.554604</td>\n",
       "      <td>2.438915</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HU.GS</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.221484</td>\n",
       "      <td>6.707251</td>\n",
       "      <td>4.234071</td>\n",
       "      <td>2.309359</td>\n",
       "      <td>7.375000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HU.KE</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.472491</td>\n",
       "      <td>6.827971</td>\n",
       "      <td>4.416804</td>\n",
       "      <td>2.356338</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HU.HE</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.646965</td>\n",
       "      <td>7.024940</td>\n",
       "      <td>4.500539</td>\n",
       "      <td>2.415034</td>\n",
       "      <td>1.791667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>HU.BZ</td>\n",
       "      <td>2010</td>\n",
       "      <td>9.298549</td>\n",
       "      <td>7.031641</td>\n",
       "      <td>4.529795</td>\n",
       "      <td>2.427436</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HU.SZ</td>\n",
       "      <td>2010</td>\n",
       "      <td>10.322904</td>\n",
       "      <td>7.094248</td>\n",
       "      <td>4.608011</td>\n",
       "      <td>2.464720</td>\n",
       "      <td>4.055556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Region  Year  avg_temp_2010  avg_r_2010  avg_iqr_2010  avg_sd_2010  \\\n",
       "0   HU.SO  2010      10.636482    6.576395      4.133575     2.259847   \n",
       "1   HU.BA  2010      10.518564    6.734973      4.366267     2.340008   \n",
       "2   HU.TO  2010      10.607542    6.669679      4.356648     2.323706   \n",
       "3   HU.BK  2010      10.859338    7.067929      4.647409     2.459515   \n",
       "4   HU.CS  2010      11.291382    7.260301      4.732937     2.514615   \n",
       "5   HU.ZA  2010      10.451497    6.604736      4.129944     2.265031   \n",
       "6   HU.BE  2010      11.307888    7.282594      4.690013     2.512147   \n",
       "7   HU.VA  2010       9.911473    6.855250      4.393657     2.369735   \n",
       "8   HU.VE  2010      10.047694    6.400322      4.051843     2.200229   \n",
       "9   HU.FE  2010       9.988019    6.603472      4.314282     2.292029   \n",
       "10  HU.PE  2010      10.121066    6.953152      4.482567     2.392689   \n",
       "11  HU.JN  2010      10.835110    7.106805      4.554490     2.447855   \n",
       "12  HU.HB  2010      10.831492    7.063108      4.554604     2.438915   \n",
       "13  HU.GS  2010      10.221484    6.707251      4.234071     2.309359   \n",
       "14  HU.KE  2010       9.472491    6.827971      4.416804     2.356338   \n",
       "15  HU.HE  2010       9.646965    7.024940      4.500539     2.415034   \n",
       "16  HU.BZ  2010       9.298549    7.031641      4.529795     2.427436   \n",
       "17  HU.SZ  2010      10.322904    7.094248      4.608011     2.464720   \n",
       "\n",
       "    avg_n30_2010  avg_n35_2010  \n",
       "0       6.388889           0.0  \n",
       "1       5.666667           0.0  \n",
       "2       6.666667           0.0  \n",
       "3       8.194444           0.0  \n",
       "4      10.854167           0.0  \n",
       "5       6.125000           0.0  \n",
       "6       8.833333           0.0  \n",
       "7       6.416667           0.0  \n",
       "8       6.125000           0.0  \n",
       "9       5.250000           0.0  \n",
       "10      4.944444           0.0  \n",
       "11      7.125000           0.0  \n",
       "12      6.166667           0.0  \n",
       "13      7.375000           0.0  \n",
       "14      3.000000           0.0  \n",
       "15      1.791667           0.0  \n",
       "16      1.950000           0.0  \n",
       "17      4.055556           0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps_by_region_2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the results based on region, year, and month\n",
    "hungary_MTUS['region_sp_code_str'] = hungary_MTUS['region_sp_code'].astype(str)\n",
    "hungary_MTUS['month_str'] = hungary_MTUS['month'].astype(str)\n",
    "hungary_MTUS['year_str'] = hungary_MTUS['year'].astype(str)\n",
    "\n",
    "#merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_hungary_MTUS = hungary_MTUS.merge(temps_by_region_1999, left_on=['region_sp_code_str', 'year_str'], right_on=['Region', 'Year'], how='left')\n",
    "merged_hungary_MTUS.drop(columns=['Region', 'Year'], inplace=True)\n",
    "# merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_2003, left_on=['region_sp_code_str', 'year_str'], right_on=['Region', 'Year'], how='left')\n",
    "# #merged_spain_MTUS.drop(columns=['Region', 'Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that the temp stats appear on each Person line, regardless of month or year\n",
    "merged_spain_MTUS['avg_temp_m'] = np.nan\n",
    "merged_spain_MTUS['avg_r_m'] = np.nan\n",
    "merged_spain_MTUS['avg_iqr_m'] = np.nan\n",
    "merged_spain_MTUS['avg_sd_m'] = np.nan\n",
    "merged_spain_MTUS['avg_n30_m'] = np.nan\n",
    "merged_spain_MTUS['avg_n35_m'] = np.nan\n",
    "    \n",
    "\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    jan_temp = row['avg_temp_jan']\n",
    "    oct_temp=row['avg_temp_oct']\n",
    "    july_temp=row['avg_temp_july']\n",
    "    april_temp=row['avg_temp_apr']\n",
    "    \n",
    "    if not pd.isna(jan_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=jan_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_jan']\n",
    "        \n",
    "    elif not pd.isna(oct_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=oct_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_oct']\n",
    "        \n",
    "    elif not pd.isna(july_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=july_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_july']\n",
    "        \n",
    "    elif not pd.isna(april_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=april_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_apr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that the temp stats appear once and on the same row as the person entry, not their diary entry\n",
    "merged_spain_MTUS['avg_temp_y'] = np.nan\n",
    "merged_spain_MTUS['avg_r_y'] = np.nan\n",
    "merged_spain_MTUS['avg_iqr_y'] = np.nan\n",
    "merged_spain_MTUS['avg_sd_y'] = np.nan\n",
    "merged_spain_MTUS['avg_n30_y'] = np.nan\n",
    "merged_spain_MTUS['avg_n35_y'] = np.nan\n",
    "    \n",
    "\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    temp_2003 = row['avg_temp_2003']\n",
    "    temp_2002=row['avg_temp_2002']\n",
    "    \n",
    "    \n",
    "    if not pd.isna(temp_2003):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_y']=temp_2003\n",
    "        merged_spain_MTUS.at[index, 'avg_r_y']=row['avg_r_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_y']=row['avg_iqr_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_y']=row['avg_sd_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_y']=row['avg_n30_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_y']=row['avg_n35_2003']\n",
    "        \n",
    "    elif not pd.isna(temp_2002):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_y']=temp_2002\n",
    "        merged_spain_MTUS.at[index, 'avg_r_y']=row['avg_r_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_y']=row['avg_iqr_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_y']=row['avg_sd_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_y']=row['avg_n30_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_y']=row['avg_n35_2002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time-use variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['rectype_str'] = merged_spain_MTUS['rectype'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['clockend_base10'] = merged_spain_MTUS['clockst_base10']+ (merged_spain_MTUS['time']/60)\n",
    "# Apply modulo operation to handle values greater than 24\n",
    "merged_spain_MTUS['clockend_base10'] = merged_spain_MTUS['clockend_base10'] % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variables for start work time, start commute time, start work time at home, and work at home\n",
    "merged_spain_MTUS['main_str'] = spain_MTUS['main'].astype(str)\n",
    "merged_spain_MTUS['sec_str'] = spain_MTUS['sec'].astype(str)\n",
    "merged_spain_MTUS.sort_values(by=['ident', 'clockst_base10'], inplace=True)\n",
    "\n",
    "# Initialize the new column\n",
    "merged_spain_MTUS['start_work_time'] = None\n",
    "# merged_spain_MTUS['end_work_time']=None\n",
    "merged_spain_MTUS['start_work_time_home'] = None\n",
    "merged_spain_MTUS['start_commute_time'] = None\n",
    "merged_spain_MTUS['work_at_home'] = 0\n",
    "\n",
    "# Create a set to keep track of indices for which \"paid work main job\" has been encountered\n",
    "people_with_paid_work = set()\n",
    "people_with_commute=set()\n",
    "people_with_paid_work_home = set()\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    current_person = row['ident']\n",
    "    main_activity = row['main_str']\n",
    "    sec_activity = row['sec_str']\n",
    "    clockst_base10 = row['clockst_base10']\n",
    "#     clockend_base10=row['clockend_base10']\n",
    "\n",
    "    if (main_activity == \"Paid work-main job (not at home)\" or sec_activity == \"Paid work-main job (not at home)\"):\n",
    "#         merged_spain_MTUS.at[index, 'end_work_time'] = clockend_base10\n",
    "        if current_person not in people_with_paid_work:\n",
    "        # Record the first occurrence of \"paid work main job\" for the current index\n",
    "            people_with_paid_work.add(current_person)\n",
    "            merged_spain_MTUS.at[index, 'start_work_time'] = clockst_base10\n",
    "        \n",
    "    if (main_activity == \"Paid work at home\" or sec_activity == \"Paid work at home\"):\n",
    "#         merged_spain_MTUS.at[index, 'end_work_time'] = clockend_base10\n",
    "        if current_person not in people_with_paid_work_home:\n",
    "            # Record the first occurrence of \"paid work main job\" for the current index\n",
    "            people_with_paid_work_home.add(current_person)\n",
    "            merged_spain_MTUS.at[index, 'start_work_time_home'] = clockst_base10\n",
    "            merged_spain_MTUS.at[index, 'work_at_home'] = 1\n",
    "    \n",
    "    if (main_activity == \"Travel to/from work\" or sec_activity == \"Travel to/from work\") and current_person not in people_with_commute:\n",
    "        # Record the first occurrence of \"paid work main job\" for the current index\n",
    "        people_with_commute.add(current_person)\n",
    "        merged_spain_MTUS.at[index, 'start_commute_time'] = clockst_base10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['start_work_time'] = merged_spain_MTUS['start_work_time'].astype(str)\n",
    "merged_spain_MTUS['start_work_time_home'] = merged_spain_MTUS['start_work_time_home'].astype(str)\n",
    "merged_spain_MTUS['start_commute_time'] = merged_spain_MTUS['start_commute_time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create total break time variable\n",
    "merged_spain_MTUS.sort_values(by=['ident', 'clockst_base10'], inplace=True)\n",
    "cumulative_break_times_peak = {}\n",
    "cumulative_break_times_offpeak = {}\n",
    "end_time_dict={}\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    current_person = row['ident']\n",
    "    main_activity = row['main_str']\n",
    "    sec_activity = row['sec_str']\n",
    "    clockst_base10 = row['clockst_base10']\n",
    "    time = row['time']\n",
    "    end_time = clockst_base10 + (time / 60)\n",
    "    clockend_base10=row['clockend_base10']\n",
    "    \n",
    "    if (main_activity == \"Paid work at home\" or sec_activity == \"Paid work at home\"):\n",
    "        end_time_dict[current_person]=clockend_base10\n",
    "    if (main_activity == \"Paid work-main job (not at home)\" or sec_activity == \"Paid work-main job (not at home)\"):\n",
    "        end_time_dict[current_person]=clockend_base10\n",
    "    if (main_activity == \"Work breaks\" or sec_activity == \"Work breaks\"):\n",
    "        if current_person not in cumulative_break_times_peak:\n",
    "            cumulative_break_times_peak[current_person] = 0\n",
    "            cumulative_break_times_offpeak[current_person] = 0\n",
    "   \n",
    "    # Check if the break falls within peak hours (12-16)\n",
    "        if 12 <= clockst_base10 <= 16:\n",
    "            if 12 <= end_time <= 16:\n",
    "                cumulative_break_times_peak[current_person] += end_time - clockst_base10\n",
    "            else:\n",
    "                cumulative_break_times_peak[current_person] += 16 - clockst_base10\n",
    "                cumulative_break_times_offpeak[current_person] += end_time - 16\n",
    "        else:\n",
    "            if 12 <= end_time <= 16:\n",
    "                cumulative_break_times_peak[current_person] += end_time - 12\n",
    "                cumulative_break_times_offpeak[current_person] += 12 - clockst_base10\n",
    "            else:\n",
    "                cumulative_break_times_offpeak[current_person] += end_time - clockst_base10\n",
    "\n",
    "grouped = pd.DataFrame({\n",
    "    'ident': list(cumulative_break_times_peak.keys()),\n",
    "    'total_break_time_peak': list(cumulative_break_times_peak.values()),\n",
    "    'total_break_time_offpeak': list(cumulative_break_times_offpeak.values())\n",
    "})\n",
    "\n",
    "grouped1= pd.DataFrame({\n",
    "    'ident': list(end_time_dict.keys()),\n",
    "    'end_work_time': list(end_time_dict.values())\n",
    "})\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the dataset with the new break time variables\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(grouped, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(grouped1, left_on=['ident'], right_on=['ident'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy = merged_spain_MTUS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy.sort_values(by=['ident', 'rectype_str', 'clockst_base10'], ascending=[True, False, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that total break time only appears once in the Person row\n",
    "merged_spain_MTUS_copy['total_break_time_peak'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['total_break_time_peak'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "merged_spain_MTUS_copy['total_break_time_offpeak'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['total_break_time_offpeak'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['end_work_time'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['end_work_time'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting again so that time use variables only appear once in the Person row of the df\n",
    "merged_spain_MTUS_copy.sort_values(by=['ident', 'rectype_str', 'clockst_base10'], ascending=[True, False, True], inplace=True)\n",
    "merged_spain_MTUS_copy.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "start_work = {}\n",
    "start_work_h={}\n",
    "start_commute={}\n",
    "work_at_h={}\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS_copy.iterrows():\n",
    "    current_person = row['ident']\n",
    "    start_work_time=row['start_work_time']\n",
    "    start_work_time_home=row['start_work_time_home'] \n",
    "    start_commute_time=row['start_commute_time'] \n",
    "    work_at_home=row['work_at_home']    \n",
    "    \n",
    "    work_at_h[current_person]=work_at_home\n",
    "    \n",
    "    if start_work_time!='None':\n",
    "        start_work[current_person]=start_work_time\n",
    "        \n",
    "    if start_work_time_home!='None':\n",
    "        start_work_h[current_person]=start_work_time_home\n",
    "        \n",
    "    if start_commute_time!='None':\n",
    "        start_commute[current_person]=start_commute_time\n",
    "    \n",
    "\n",
    "start_w = pd.DataFrame({\n",
    "    'ident': list(start_work.keys()),\n",
    "    'start_work_time1': list(start_work.values()),\n",
    "})\n",
    "\n",
    "start_wh = pd.DataFrame({\n",
    "    'ident': list(start_work_h.keys()),\n",
    "    'start_work_time_home1': list(start_work_h.values()),\n",
    "})\n",
    "\n",
    "start_com = pd.DataFrame({\n",
    "    'ident': list(start_commute.keys()),\n",
    "    'start_commute_time1': list(start_commute.values()),\n",
    "})\n",
    "\n",
    "work_ah = pd.DataFrame({\n",
    "    'ident': list(work_at_h.keys()),\n",
    "    'work_at_home1': list(work_at_h.values()),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge for formating so that the time use variables appear in every row of that Person\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_w, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_wh, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_com, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(work_ah, left_on=['ident'], right_on=['ident'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format so that the time use variables only appear once in the Person row, not in the subsequent Diary rows\n",
    "merged_spain_MTUS_copy['start_work_time1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_work_time1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['start_work_time_home1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_work_time_home1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['start_commute_time1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_commute_time1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['work_at_home1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['work_at_home1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy.drop(columns=['work_at_home','start_commute_time' , 'start_work_time_home', 'start_work_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.rename(columns={'work_at_home1': 'work_at_home', 'start_commute_time1': 'start_commute_time', 'start_work_time_home1': 'start_work_time_home', 'start_work_time1': 'start_work_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy['start_commute_time'] = merged_spain_MTUS_copy['start_commute_time'].astype(float)\n",
    "merged_spain_MTUS_copy['start_work_time_home'] = merged_spain_MTUS_copy['start_work_time_home'].astype(float)\n",
    "merged_spain_MTUS_copy['start_work_time'] = merged_spain_MTUS_copy['start_work_time'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final merge to stata\n",
    "merged_spain_MTUS_copy.to_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Spain_FINAL.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy\n",
    "!pip install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "df=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Spain_UPDATED_FINAL.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_sp_str'] = df['region_sp'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique=df['region_sp_str'].unique()\n",
    "unique_val=[]\n",
    "\n",
    "for i in unique:\n",
    "    if i!='nan':\n",
    "        unique_val.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HOTTEST REGIONS FOR REFERENCE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "avg_temp_by_region = df.groupby('region_sp_str')['avg_temp_y'].mean()\n",
    "sorted_regions = avg_temp_by_region.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for start time and avg_temp\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['start_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Hottest to Coldest)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of Start Work Time Within Spain')\n",
    "plt.xlabel('Start Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Starttime.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar graph, but for end time and avg_temp\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['end_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Hottest to Coldest)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of End Work Time Within Spain')\n",
    "plt.xlabel('End Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Endtime.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same but for n30, number of days above 30 degrees\n",
    "\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "avg_n30_by_region = df.groupby('region_sp_str')['avg_n30_y'].mean()\n",
    "sorted_regions_n30 = avg_n30_by_region.sort_values(ascending=False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for start time and n30\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions_n30)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions_n30, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions_n30:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['start_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Most to Least Days>=30)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of Start Work Time Within Spain')\n",
    "plt.xlabel('Start Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Starttime_n30.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for end time and n30\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions_n30)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions_n30, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions_n30:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['end_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Most to Least Days>=30)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of End Work Time Within Spain')\n",
    "plt.xlabel('End Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Endtime_n30.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
