{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Overall imports/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joyse\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the temp files\n",
    "temp_jan=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_01.dta\")\n",
    "temp_feb=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_02.dta\")\n",
    "temp_mar=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_03.dta\")\n",
    "temp_apr=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_04.dta\")\n",
    "temp_may=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_05.dta\")\n",
    "temp_june=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_06.dta\")\n",
    "temp_july=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_07.dta\")\n",
    "temp_aug=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_08.dta\")\n",
    "temp_sept=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_09.dta\")\n",
    "temp_oct=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_10.dta\")\n",
    "temp_nov=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_11.dta\")\n",
    "temp_dec=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\monthlyPanelsTemp\\\\2m_temperature_1950_2019_12.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Stata dataset into a DataFrame\n",
    "lat_long_match = pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\QGIS\\\\final_LatLong_match.dta\")\n",
    "fin_MTUS=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Finland.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all lat/long points in Spain\n",
    "fin_match = lat_long_match[lat_long_match['name_0'] == 'Finland']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create country codes in MTUS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a dictionary mapping abbreviations to state names\n",
    "# for index, row in nl_match.iterrows():\n",
    "#     if row['name_1'] == 'Zuid Hollandse Meren':\n",
    "#         nl_match.at[index, 'hasc_1'] = 'NL.ZH'\n",
    "\n",
    "mapping_dict = dict(zip(fin_match['hasc_1'], fin_match['name_1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FI.LS': 'Western Finland',\n",
       " 'FI.ES': 'Southern Finland',\n",
       " 'FI.IS': 'Eastern Finland',\n",
       " 'FI.OU': 'Oulu',\n",
       " 'FI.LP': 'Lapland'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print dictionary to verify\n",
    "mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_values(['Western Finland', 'Southern Finland', 'Eastern Finland', 'Oulu', 'Lapland'])\n"
     ]
    }
   ],
   "source": [
    "print(mapping_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rectype', 'sample', 'ident', 'country', 'hldid', 'persid', 'diary',\n",
       "       'year', 'diaryid', 'relrefp', 'day', 'month', 'hhtype', 'hhldsize',\n",
       "       'nchild', 'famstat', 'singpar', 'vehicle', 'computer', 'urban',\n",
       "       'agekid', 'agekid2', 'age', 'sex', 'civstat', 'cohab', 'edtry', 'educa',\n",
       "       'carer', 'empstat', 'empsp', 'sector', 'emp', 'unemp', 'workhrs',\n",
       "       'retired', 'student', 'health', 'region', 'disab', 'income', 'incorig',\n",
       "       'isco1', 'ocombwt', 'propwt', 'region_fi', 'diarytype', 'badcase',\n",
       "       'act_chcare', 'act_civic', 'act_educa', 'act_inhome', 'act_media',\n",
       "       'act_norec', 'act_outhome', 'act_pcare', 'act_physical', 'act_travel',\n",
       "       'act_undom', 'act_work', 'epnum', 'clockst', 'start', 'end', 'time',\n",
       "       'main', 'sec', 'eloc', 'inout', 'mtrav', 'alone', 'alone_alt', 'child',\n",
       "       'sppart', 'oad', 'ict'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_MTUS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region_fi\n",
       "Rest of southern Finland    2347\n",
       "Western Finland             2219\n",
       "Greater Helsinki            1058\n",
       "Eastern Finland             1012\n",
       "Northern Finland             819\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the regions in the Spain MTUS dataset\n",
    "fin_MTUS['region_fi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Western Finland': 'FI.LS',\n",
       " 'Southern Finland': 'FI.ES',\n",
       " 'Eastern Finland': 'FI.IS',\n",
       " 'Oulu': 'FI.OU',\n",
       " 'Lapland': 'FI.LP'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Flip the value of our earlier dictionary for easier matching later\n",
    "flipped_dict = {value: key for key, value in mapping_dict.items()}\n",
    "flipped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edditing the final dict based on the syntax from spain_MTUS\n",
    "final_dict={'Eastern Finland': 'FI.IS', 'Western Finland': 'FI.LS'\n",
    "           }\n",
    "\n",
    "\n",
    "# Rest of southern Finland    2347\n",
    "# Greater Helsinki            1058\n",
    "# Northern Finland             819\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Eastern Finland', 'Western Finland'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new variable in Spain MTUS with the corresponding abbreviation, useful for later merge\n",
    "spain_MTUS['region_sp_code'] = spain_MTUS['region_sp'].map(final_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the start time to base 10\n",
    "spain_MTUS['clockst_base10'] = spain_MTUS['clockst'].apply(lambda x: int(x) + (x % 1) / 0.6 if not pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all lat/long points by location and merge temperature data into MTUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the lat, long values within a given region\n",
    "points_by_state = {}\n",
    "\n",
    "# Iterate over each state code in the dictionary\n",
    "for state_code in mapping_dict.keys():\n",
    "    # Filter the DataFrame for points in the current region\n",
    "    region_points = spain_match.loc[spain_match['hasc_1'] == state_code, ['latitude', 'longitude']]\n",
    "    \n",
    "    # Convert the filtered DataFrame to a list of tuples\n",
    "    points_list = list(zip(region_points['latitude'], region_points['longitude']))\n",
    "    \n",
    "    # Store the list of points in the dictionary\n",
    "    points_by_state[state_code] = points_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display points_by_state dictionary to verify\n",
    "points_by_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells get the average temperature for specific months of surveys, if monthly info not available, proceed to next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for January\n",
    "year=2003\n",
    "month=\"January\"\n",
    "temps_by_region_jan = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_jan', \"avg_r_jan\", \n",
    "                                       \"avg_iqr_jan\", \"avg_sd_jan\", \n",
    "                                        \"avg_n30_jan\", \"avg_n35_jan\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_jan[(temp_jan['latitude'] == lat) & \n",
    "                             (temp_jan['longitude'] == long) & \n",
    "                             (temp_jan['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month1'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            #num_above_30_list.extend(temp_data['wavg_25_ct'].tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_jan.loc[len(temps_by_region_jan.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for April\n",
    "year=2003\n",
    "month=\"April\"\n",
    "temps_by_region_apr = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_apr', \"avg_r_apr\", \n",
    "                                       \"avg_iqr_apr\", \"avg_sd_apr\", \n",
    "                                        \"avg_n30_apr\", \"avg_n35_apr\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_apr[(temp_apr['latitude'] == lat) & \n",
    "                             (temp_apr['longitude'] == long) & \n",
    "                             (temp_apr['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month4'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_apr.loc[len(temps_by_region_apr.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for October\n",
    "year=2002\n",
    "month=\"October\"\n",
    "temps_by_region_oct = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_oct', \"avg_r_oct\", \n",
    "                                       \"avg_iqr_oct\", \"avg_sd_oct\", \n",
    "                                        \"avg_n30_oct\", \"avg_n35_oct\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_oct[(temp_oct['latitude'] == lat) & \n",
    "                             (temp_oct['longitude'] == long) & \n",
    "                             (temp_oct['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month10'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)] + ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_oct.loc[len(temps_by_region_oct.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_oct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2003\n",
    "month=\"July\"\n",
    "temps_by_region_july = pd.DataFrame(columns=['Region', \"Year\", \"Month\", 'avg_temp_july', \"avg_r_july\", \n",
    "                                       \"avg_iqr_july\", \"avg_sd_july\", \n",
    "                                        \"avg_n30_july\", \"avg_n35_july\"])\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for point in points_by_state[key]:\n",
    "        temp_hour_list=[]\n",
    "        lat,long=point[0], point[1]\n",
    "        temp_data = temp_july[(temp_july['latitude'] == lat) & \n",
    "                             (temp_july['longitude'] == long) & \n",
    "                             (temp_july['year'] == year)]\n",
    "        \n",
    "        # Check if any data is found for the point and year\n",
    "        if not temp_data.empty:\n",
    "            avg_temp_month_list.extend(temp_data['avg_temp_month7'].tolist())\n",
    "            num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "            for hour in range(1, 25):\n",
    "                temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "        \n",
    "\n",
    "        if temp_hour_list:\n",
    "            iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "            std_list.append(np.std(temp_hour_list))\n",
    "            range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "             \n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "    \n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_july.loc[len(temps_by_region_july.index)] = [key, str(year), month, avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify dataframe\n",
    "temps_by_region_july"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the results based on region, year, and month\n",
    "spain_MTUS['region_sp_code_str'] = spain_MTUS['region_sp_code'].astype(str)\n",
    "spain_MTUS['month_str'] = spain_MTUS['month'].astype(str)\n",
    "spain_MTUS['year_str'] = spain_MTUS['year'].astype(str)\n",
    "\n",
    "\n",
    "merged_spain_MTUS = spain_MTUS.merge(temps_by_region_jan, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_apr, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_july, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_oct, left_on=['region_sp_code_str', 'year_str', 'month_str'], right_on=['Region', 'Year', 'Month'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following gets the temperature stats for the year(s) of the survey, not the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2002\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_2002 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_2002', \"avg_r_2002\", \n",
    "                                       \"avg_iqr_2002\", \"avg_sd_2002\", \n",
    "                                        \"avg_n30_2002\", \"avg_n35_2002\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_2002.loc[len(temps_by_region_2002.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_by_region_2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new df with the stats we want on the temperature based on location for July\n",
    "year=2003\n",
    "months=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec']\n",
    "temps_by_region_2003 = pd.DataFrame(columns=['Region', \"Year\", 'avg_temp_2003', \"avg_r_2003\", \n",
    "                                       \"avg_iqr_2003\", \"avg_sd_2003\", \n",
    "                                        \"avg_n30_2003\", \"avg_n35_2003\"])\n",
    "\n",
    "for key in points_by_state.keys():\n",
    "    avg_temp_month_list=[]\n",
    "    num_above_30_list=[]\n",
    "    num_above_35_list=[]\n",
    "    iqr_list=[]\n",
    "    range_list=[]\n",
    "    std_list=[]\n",
    "    for i in months:\n",
    "        for point in points_by_state[key]:\n",
    "            temp_hour_list=[]\n",
    "            lat,long=point[0], point[1]\n",
    "            temp_data = globals()[f'temp_{i}'][(globals()[f'temp_{i}']['latitude'] == lat) &\n",
    "                                   (globals()[f'temp_{i}']['longitude'] == long) &\n",
    "                                   (globals()[f'temp_{i}']['year'] == year)]\n",
    "\n",
    "\n",
    "        # Check if any data is found for the point and year\n",
    "\n",
    "            if not temp_data.empty:\n",
    "                month_index = months.index(i) + 1\n",
    "                avg_temp_name = f'avg_temp_month{month_index}'\n",
    "                avg_temp_month_list.extend(temp_data[avg_temp_name].tolist())\n",
    "                num_above_30_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(30, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                num_above_35_list.extend(temp_data[['num_hrs_{}_ct'.format(i) for i in range(35, 45)]+ ['num_hrs_g45_ct']].sum(axis=1).tolist())\n",
    "                for hour in range(1, 25):\n",
    "                    temp_hour_list.extend(temp_data[f'avg_temp_hr{hour}'].tolist())\n",
    "\n",
    "\n",
    "            if temp_hour_list:\n",
    "                iqr_list.append(np.percentile(temp_hour_list, 75) - np.percentile(temp_hour_list, 25))\n",
    "                std_list.append(np.std(temp_hour_list))\n",
    "                range_list.append(np.max(temp_hour_list) - np.min(temp_hour_list))\n",
    "    \n",
    "    #Get the averages for all points in a region over all months in a year\n",
    "    avg_temp_region = np.mean(avg_temp_month_list) if avg_temp_month_list else 0\n",
    "    range_region = np.mean(range_list) if range_list else 0\n",
    "    iqr_region = np.mean(iqr_list) if iqr_list else 0\n",
    "    std_dev_region = np.mean(std_list) if std_list else 0\n",
    "\n",
    "    avg_num_30_region=np.mean(num_above_30_list)\n",
    "    avg_num_35_region=np.mean(num_above_35_list)\n",
    " \n",
    "    temps_by_region_2003.loc[len(temps_by_region_2003.index)] = [key, str(year), avg_temp_region, range_region, iqr_region,std_dev_region,\n",
    "                                                      avg_num_30_region, avg_num_35_region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_by_region_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the results based on region, year, and month\n",
    "# spain_MTUS['region_sp_code_str'] = spain_MTUS['region_sp_code'].astype(str)\n",
    "# spain_MTUS['month_str'] = spain_MTUS['month'].astype(str)\n",
    "# spain_MTUS['year_str'] = spain_MTUS['year'].astype(str)\n",
    "\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year', 'Month'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_2002, left_on=['region_sp_code_str', 'year_str'], right_on=['Region', 'Year'], how='left')\n",
    "merged_spain_MTUS.drop(columns=['Region', 'Year'], inplace=True)\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(temps_by_region_2003, left_on=['region_sp_code_str', 'year_str'], right_on=['Region', 'Year'], how='left')\n",
    "#merged_spain_MTUS.drop(columns=['Region', 'Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that the temp stats appear on each Person line, regardless of month or year\n",
    "merged_spain_MTUS['avg_temp_m'] = np.nan\n",
    "merged_spain_MTUS['avg_r_m'] = np.nan\n",
    "merged_spain_MTUS['avg_iqr_m'] = np.nan\n",
    "merged_spain_MTUS['avg_sd_m'] = np.nan\n",
    "merged_spain_MTUS['avg_n30_m'] = np.nan\n",
    "merged_spain_MTUS['avg_n35_m'] = np.nan\n",
    "    \n",
    "\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    jan_temp = row['avg_temp_jan']\n",
    "    oct_temp=row['avg_temp_oct']\n",
    "    july_temp=row['avg_temp_july']\n",
    "    april_temp=row['avg_temp_apr']\n",
    "    \n",
    "    if not pd.isna(jan_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=jan_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_jan']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_jan']\n",
    "        \n",
    "    elif not pd.isna(oct_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=oct_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_oct']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_oct']\n",
    "        \n",
    "    elif not pd.isna(july_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=july_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_july']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_july']\n",
    "        \n",
    "    elif not pd.isna(april_temp):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_m']=april_temp\n",
    "        merged_spain_MTUS.at[index, 'avg_r_m']=row['avg_r_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_m']=row['avg_iqr_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_m']=row['avg_sd_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_m']=row['avg_n30_apr']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_m']=row['avg_n35_apr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that the temp stats appear once and on the same row as the person entry, not their diary entry\n",
    "merged_spain_MTUS['avg_temp_y'] = np.nan\n",
    "merged_spain_MTUS['avg_r_y'] = np.nan\n",
    "merged_spain_MTUS['avg_iqr_y'] = np.nan\n",
    "merged_spain_MTUS['avg_sd_y'] = np.nan\n",
    "merged_spain_MTUS['avg_n30_y'] = np.nan\n",
    "merged_spain_MTUS['avg_n35_y'] = np.nan\n",
    "    \n",
    "\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    temp_2003 = row['avg_temp_2003']\n",
    "    temp_2002=row['avg_temp_2002']\n",
    "    \n",
    "    \n",
    "    if not pd.isna(temp_2003):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_y']=temp_2003\n",
    "        merged_spain_MTUS.at[index, 'avg_r_y']=row['avg_r_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_y']=row['avg_iqr_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_y']=row['avg_sd_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_y']=row['avg_n30_2003']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_y']=row['avg_n35_2003']\n",
    "        \n",
    "    elif not pd.isna(temp_2002):\n",
    "        merged_spain_MTUS.at[index, 'avg_temp_y']=temp_2002\n",
    "        merged_spain_MTUS.at[index, 'avg_r_y']=row['avg_r_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_iqr_y']=row['avg_iqr_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_sd_y']=row['avg_sd_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_n30_y']=row['avg_n30_2002']\n",
    "        merged_spain_MTUS.at[index, 'avg_n35_y']=row['avg_n35_2002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create time-use variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['rectype_str'] = merged_spain_MTUS['rectype'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['clockend_base10'] = merged_spain_MTUS['clockst_base10']+ (merged_spain_MTUS['time']/60)\n",
    "# Apply modulo operation to handle values greater than 24\n",
    "merged_spain_MTUS['clockend_base10'] = merged_spain_MTUS['clockend_base10'] % 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create variables for start work time, start commute time, start work time at home, and work at home\n",
    "merged_spain_MTUS['main_str'] = spain_MTUS['main'].astype(str)\n",
    "merged_spain_MTUS['sec_str'] = spain_MTUS['sec'].astype(str)\n",
    "merged_spain_MTUS.sort_values(by=['ident', 'clockst_base10'], inplace=True)\n",
    "\n",
    "# Initialize the new column\n",
    "merged_spain_MTUS['start_work_time'] = None\n",
    "# merged_spain_MTUS['end_work_time']=None\n",
    "merged_spain_MTUS['start_work_time_home'] = None\n",
    "merged_spain_MTUS['start_commute_time'] = None\n",
    "merged_spain_MTUS['work_at_home'] = 0\n",
    "\n",
    "# Create a set to keep track of indices for which \"paid work main job\" has been encountered\n",
    "people_with_paid_work = set()\n",
    "people_with_commute=set()\n",
    "people_with_paid_work_home = set()\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    current_person = row['ident']\n",
    "    main_activity = row['main_str']\n",
    "    sec_activity = row['sec_str']\n",
    "    clockst_base10 = row['clockst_base10']\n",
    "#     clockend_base10=row['clockend_base10']\n",
    "\n",
    "    if (main_activity == \"Paid work-main job (not at home)\" or sec_activity == \"Paid work-main job (not at home)\"):\n",
    "#         merged_spain_MTUS.at[index, 'end_work_time'] = clockend_base10\n",
    "        if current_person not in people_with_paid_work:\n",
    "        # Record the first occurrence of \"paid work main job\" for the current index\n",
    "            people_with_paid_work.add(current_person)\n",
    "            merged_spain_MTUS.at[index, 'start_work_time'] = clockst_base10\n",
    "        \n",
    "    if (main_activity == \"Paid work at home\" or sec_activity == \"Paid work at home\"):\n",
    "#         merged_spain_MTUS.at[index, 'end_work_time'] = clockend_base10\n",
    "        if current_person not in people_with_paid_work_home:\n",
    "            # Record the first occurrence of \"paid work main job\" for the current index\n",
    "            people_with_paid_work_home.add(current_person)\n",
    "            merged_spain_MTUS.at[index, 'start_work_time_home'] = clockst_base10\n",
    "            merged_spain_MTUS.at[index, 'work_at_home'] = 1\n",
    "    \n",
    "    if (main_activity == \"Travel to/from work\" or sec_activity == \"Travel to/from work\") and current_person not in people_with_commute:\n",
    "        # Record the first occurrence of \"paid work main job\" for the current index\n",
    "        people_with_commute.add(current_person)\n",
    "        merged_spain_MTUS.at[index, 'start_commute_time'] = clockst_base10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS['start_work_time'] = merged_spain_MTUS['start_work_time'].astype(str)\n",
    "merged_spain_MTUS['start_work_time_home'] = merged_spain_MTUS['start_work_time_home'].astype(str)\n",
    "merged_spain_MTUS['start_commute_time'] = merged_spain_MTUS['start_commute_time'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create total break time variable\n",
    "merged_spain_MTUS.sort_values(by=['ident', 'clockst_base10'], inplace=True)\n",
    "cumulative_break_times_peak = {}\n",
    "cumulative_break_times_offpeak = {}\n",
    "end_time_dict={}\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS.iterrows():\n",
    "    current_person = row['ident']\n",
    "    main_activity = row['main_str']\n",
    "    sec_activity = row['sec_str']\n",
    "    clockst_base10 = row['clockst_base10']\n",
    "    time = row['time']\n",
    "    end_time = clockst_base10 + (time / 60)\n",
    "    clockend_base10=row['clockend_base10']\n",
    "    \n",
    "    if (main_activity == \"Paid work at home\" or sec_activity == \"Paid work at home\"):\n",
    "        end_time_dict[current_person]=clockend_base10\n",
    "    if (main_activity == \"Paid work-main job (not at home)\" or sec_activity == \"Paid work-main job (not at home)\"):\n",
    "        end_time_dict[current_person]=clockend_base10\n",
    "    if (main_activity == \"Work breaks\" or sec_activity == \"Work breaks\"):\n",
    "        if current_person not in cumulative_break_times_peak:\n",
    "            cumulative_break_times_peak[current_person] = 0\n",
    "            cumulative_break_times_offpeak[current_person] = 0\n",
    "   \n",
    "    # Check if the break falls within peak hours (12-16)\n",
    "        if 12 <= clockst_base10 <= 16:\n",
    "            if 12 <= end_time <= 16:\n",
    "                cumulative_break_times_peak[current_person] += end_time - clockst_base10\n",
    "            else:\n",
    "                cumulative_break_times_peak[current_person] += 16 - clockst_base10\n",
    "                cumulative_break_times_offpeak[current_person] += end_time - 16\n",
    "        else:\n",
    "            if 12 <= end_time <= 16:\n",
    "                cumulative_break_times_peak[current_person] += end_time - 12\n",
    "                cumulative_break_times_offpeak[current_person] += 12 - clockst_base10\n",
    "            else:\n",
    "                cumulative_break_times_offpeak[current_person] += end_time - clockst_base10\n",
    "\n",
    "grouped = pd.DataFrame({\n",
    "    'ident': list(cumulative_break_times_peak.keys()),\n",
    "    'total_break_time_peak': list(cumulative_break_times_peak.values()),\n",
    "    'total_break_time_offpeak': list(cumulative_break_times_offpeak.values())\n",
    "})\n",
    "\n",
    "grouped1= pd.DataFrame({\n",
    "    'ident': list(end_time_dict.keys()),\n",
    "    'end_work_time': list(end_time_dict.values())\n",
    "})\n",
    "\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the dataset with the new break time variables\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(grouped, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS = merged_spain_MTUS.merge(grouped1, left_on=['ident'], right_on=['ident'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy = merged_spain_MTUS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy.sort_values(by=['ident', 'rectype_str', 'clockst_base10'], ascending=[True, False, True], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting so that total break time only appears once in the Person row\n",
    "merged_spain_MTUS_copy['total_break_time_peak'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['total_break_time_peak'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "merged_spain_MTUS_copy['total_break_time_offpeak'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['total_break_time_offpeak'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['end_work_time'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['end_work_time'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting again so that time use variables only appear once in the Person row of the df\n",
    "merged_spain_MTUS_copy.sort_values(by=['ident', 'rectype_str', 'clockst_base10'], ascending=[True, False, True], inplace=True)\n",
    "merged_spain_MTUS_copy.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "start_work = {}\n",
    "start_work_h={}\n",
    "start_commute={}\n",
    "work_at_h={}\n",
    "\n",
    "# Iterate through the rows\n",
    "for index, row in merged_spain_MTUS_copy.iterrows():\n",
    "    current_person = row['ident']\n",
    "    start_work_time=row['start_work_time']\n",
    "    start_work_time_home=row['start_work_time_home'] \n",
    "    start_commute_time=row['start_commute_time'] \n",
    "    work_at_home=row['work_at_home']    \n",
    "    \n",
    "    work_at_h[current_person]=work_at_home\n",
    "    \n",
    "    if start_work_time!='None':\n",
    "        start_work[current_person]=start_work_time\n",
    "        \n",
    "    if start_work_time_home!='None':\n",
    "        start_work_h[current_person]=start_work_time_home\n",
    "        \n",
    "    if start_commute_time!='None':\n",
    "        start_commute[current_person]=start_commute_time\n",
    "    \n",
    "\n",
    "start_w = pd.DataFrame({\n",
    "    'ident': list(start_work.keys()),\n",
    "    'start_work_time1': list(start_work.values()),\n",
    "})\n",
    "\n",
    "start_wh = pd.DataFrame({\n",
    "    'ident': list(start_work_h.keys()),\n",
    "    'start_work_time_home1': list(start_work_h.values()),\n",
    "})\n",
    "\n",
    "start_com = pd.DataFrame({\n",
    "    'ident': list(start_commute.keys()),\n",
    "    'start_commute_time1': list(start_commute.values()),\n",
    "})\n",
    "\n",
    "work_ah = pd.DataFrame({\n",
    "    'ident': list(work_at_h.keys()),\n",
    "    'work_at_home1': list(work_at_h.values()),\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge for formating so that the time use variables appear in every row of that Person\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_w, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_wh, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(start_com, left_on=['ident'], right_on=['ident'], how='left')\n",
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.merge(work_ah, left_on=['ident'], right_on=['ident'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format so that the time use variables only appear once in the Person row, not in the subsequent Diary rows\n",
    "merged_spain_MTUS_copy['start_work_time1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_work_time1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['start_work_time_home1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_work_time_home1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['start_commute_time1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['start_commute_time1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "merged_spain_MTUS_copy['work_at_home1'] = merged_spain_MTUS_copy.apply(\n",
    "    lambda row: row['work_at_home1'] if row['rectype_str'] == 'Person' else None,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy.drop(columns=['work_at_home','start_commute_time' , 'start_work_time_home', 'start_work_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy = merged_spain_MTUS_copy.rename(columns={'work_at_home1': 'work_at_home', 'start_commute_time1': 'start_commute_time', 'start_work_time_home1': 'start_work_time_home', 'start_work_time1': 'start_work_time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_spain_MTUS_copy['start_commute_time'] = merged_spain_MTUS_copy['start_commute_time'].astype(float)\n",
    "merged_spain_MTUS_copy['start_work_time_home'] = merged_spain_MTUS_copy['start_work_time_home'].astype(float)\n",
    "merged_spain_MTUS_copy['start_work_time'] = merged_spain_MTUS_copy['start_work_time'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final merge to stata\n",
    "merged_spain_MTUS_copy.to_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Spain_FINAL.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy\n",
    "!pip install numpy==1.23.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "df=pd.read_stata(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\MTUS_Spain_UPDATED_FINAL.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_sp_str'] = df['region_sp'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique=df['region_sp_str'].unique()\n",
    "unique_val=[]\n",
    "\n",
    "for i in unique:\n",
    "    if i!='nan':\n",
    "        unique_val.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HOTTEST REGIONS FOR REFERENCE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "avg_temp_by_region = df.groupby('region_sp_str')['avg_temp_y'].mean()\n",
    "sorted_regions = avg_temp_by_region.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for start time and avg_temp\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['start_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Hottest to Coldest)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of Start Work Time Within Spain')\n",
    "plt.xlabel('Start Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Starttime.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar graph, but for end time and avg_temp\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['end_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Hottest to Coldest)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of End Work Time Within Spain')\n",
    "plt.xlabel('End Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Endtime.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the same but for n30, number of days above 30 degrees\n",
    "\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "avg_n30_by_region = df.groupby('region_sp_str')['avg_n30_y'].mean()\n",
    "sorted_regions_n30 = avg_n30_by_region.sort_values(ascending=False).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for start time and n30\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions_n30)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions_n30, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions_n30:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['start_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Most to Least Days>=30)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of Start Work Time Within Spain')\n",
    "plt.xlabel('Start Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Starttime_n30.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph for end time and n30\n",
    "# Assuming df is your DataFrame with 'region_sp_str' and 'avg_temp_y' columns\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Number of regions\n",
    "num_regions = len(sorted_regions_n30)\n",
    "\n",
    "# Create a reversed color gradient from red to blue\n",
    "colors = sns.color_palette(\"RdBu_r\", n_colors=num_regions)[::-1]\n",
    "\n",
    "# Create a dictionary to map regions to colors\n",
    "region_color_dict = dict(zip(sorted_regions_n30, colors))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for region in sorted_regions_n30:\n",
    "    # Subset to the region\n",
    "    subset = df[df['region_sp_str'] == region]\n",
    "    \n",
    "    # Draw the density plot using kdeplot with color based on avg_temp_y\n",
    "    sns.kdeplot(subset['end_work_time'], label=region, color=region_color_dict[region], linewidth=1, warn_singular=False)\n",
    "\n",
    "plt.legend(prop={'size': 10}, title='Regions (Most to Least Days>=30)', bbox_to_anchor=(1, 1), loc='upper left', facecolor='white')\n",
    "plt.title('Density Plot of End Work Time Within Spain')\n",
    "plt.xlabel('End Work Time (24 Hours)')\n",
    "plt.xlim(0, 24)\n",
    "plt.xticks(range(25))\n",
    "plt.tick_params(axis='x', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "plt.tick_params(axis='y', which='both', bottom=True, top=False, labelbottom=True)  # Hide x-axis tick labels\n",
    "\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Save the plot as an image (e.g., JPG)\n",
    "plt.savefig(\"C:\\\\Users\\\\joyse\\\\Desktop\\\\UROP\\\\Fall 2023\\\\MTUS\\\\Spain_Endtime_n30.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
